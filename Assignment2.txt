import numpy as np
import time
import heapq
import random
from collections import defaultdict


# SUBMIT YOUR CODE AS A SINGLE PYTHON (.PY) FILE INSIDE A ZIP ARCHIVE
# THE NAME OF THE PYTHON FILE MUST BE submit.py

# DO NOT PERFORM ANY FILE IO IN YOUR CODE

# DO NOT CHANGE THE NAME OF THE METHOD my_fit or my_predict BELOW
# IT WILL BE INVOKED BY THE EVALUATION SCRIPT
# CHANGING THE NAME WILL CAUSE EVALUATION FAILURE

# You may define any new functions, variables, classes here
# For example, classes to create the Tree, Nodes etc
def read_words_from_file(filename):
    with open(filename, 'r') as file:
        words = file.read().splitlines()
    return words

def initialize_nested_dict():
    """Initialize a nested defaultdict of float."""
    return defaultdict(float)

def create_bigrams(word):
    """Create bigrams from a given word."""
    return [''.join(pair) for pair in zip(word[:-1], word[1:])]

def calculate_bigram_frequencies(words):
    """Calculate frequencies of bigrams within each word."""
    word_count = defaultdict(int)
    bigram_count = defaultdict(lambda: defaultdict(int))
    
    for word in words:
        word_count[word] += 1
        bigrams = create_bigrams(word)
        for bigram in bigrams:
            bigram_count[word][bigram] += 1
    
    return word_count, bigram_count

def train_bayesian_model(words):
    """Train a Bayesian model on the given dictionary of words."""
    start_time = time.time()
    word_count, bigram_count = calculate_bigram_frequencies(words)
    vocabulary_size = len(word_count)
    total_words = len(words)
    
    # Calculate prior probabilities
    prior_prob = {word: count / total_words for word, count in word_count.items()}
    
    # Calculate likelihood probabilities considering structured bigrams
    # likelihood_prob = defaultdict(lambda: defaultdict(float))
    likelihood_prob = defaultdict(initialize_nested_dict)
    for word, bigrams in bigram_count.items():
        total_bigrams = sum(bigrams.values())
        for bigram, count in bigrams.items():
            likelihood_prob[word][bigram] = count / total_bigrams
    
    end_time = time.time()
    training_time = end_time - start_time
    return prior_prob, likelihood_prob

# def predict_word(bigram_list, prior_prob, likelihood_prob):
#     """Predict the word based on the given list of bigrams using Bayesian model."""
#     max_posterior_prob = -1
#     predicted_word = None
#     for word, prior in prior_prob.items():
#         posterior_prob = prior
#         for bigram in bigram_list:
#             if bigram in likelihood_prob[word]:
#                 posterior_prob *= likelihood_prob[word][bigram]
#             else:
#                 posterior_prob = 0  # If any bigram is unseen in training data, set posterior to 0
#                 break
        
#         if posterior_prob > max_posterior_prob:
#             max_posterior_prob = posterior_prob
#             predicted_word = word
    
#     return predicted_word
def predict_word(bigram_list, prior_prob, likelihood_prob):
    """Predict the top 5 words based on the given list of bigrams using Bayesian model."""
    word_probabilities = []

    for word, prior in prior_prob.items():
        posterior_prob = prior
        for bigram in bigram_list:
            if bigram in likelihood_prob[word]:
                posterior_prob *= likelihood_prob[word][bigram]
            else:
                posterior_prob = 0  # If any bigram is unseen in training data, set posterior to 0
                break

        if posterior_prob > 0:
            if len(word_probabilities) < 5:
                heapq.heappush(word_probabilities, (posterior_prob, word))
            else:
                heapq.heappushpop(word_probabilities, (posterior_prob, word))

    # Extract the words from the heap and sort them in descending order of probability
    top_5_words = [word for _, word in sorted(word_probabilities, key=lambda x: -x[0])]
    # return top_5_words
    
    # Refine the top 5 words based on bigram matching
    refined_words = []
    sorted_given_bigrams = sorted(set(bigram_list))
    for word in top_5_words:
        # print(type(word))
        word_bigrams = sorted(set(create_bigrams(word)))[:5]
        if word_bigrams == sorted_given_bigrams:
            refined_words.append(word)
    # print(type(refined_words))
    # print(refined_words)
    # print(refined_words[0])
    return refined_words

################################
# Non Editable Region Starting #
################################
def my_fit( words ):
################################
#  Non Editable Region Ending  #
################################
	# Do not perform any file IO in your code
	# Use this method to train your model using the word list provided
	prior_prob, likelihood_prob = train_bayesian_model(words)
	model = (prior_prob, likelihood_prob)
	return model					# Return the trained model


################################
# Non Editable Region Starting #
################################
def my_predict( model, bigram_list ):
################################
#  Non Editable Region Ending  #
################################
	# Do not perform any file IO in your code
	# Use this method to predict on a test bigram_list
	# Ensure that you return a list even if making a single guess
	prior_prob, likelihood_prob = model
	predicted_word = predict_word(bigram_list, prior_prob, likelihood_prob)
	return predicted_word 					# Return guess(es) as a list
